{
   "modelname":"1_config_lemma",
   "modelpath":"./",
   "run_test":false,
   "input_path":"output/train/lemmat_full.tsv",
   "test_path":"output/test/lemmat_full.tsv",
   "dev_path":"output/dev/lemmat_full.tsv",
   "header":true,
   "sep":"\t",
   "breakline_ref":"lemma",
   "breakline_data":"NONE",
   "char_max_size":500,
   "word_max_size":20000,
   "max_sent_len":35,
   "max_sents":1000000,
   "char_min_freq":1,
   "word_min_freq":1,
   "char_eos":true,
   "char_bos":true,
   "char_lower":false,
   "tasks":[
      {
         "name":"lemma",
         "target":true,
         "context":"sentence",
         "level":"char",
         "decoder":"attentional",
         "settings":{
            "bos":true,
            "eos":true,
            "lower":false,
            "target":"lemma"
         },
         "layer":-1
      }
   ],
   "task_defaults":{
      "level":"token",
      "layer":-1,
      "decoder":"linear",
      "context":"sentence"
   },
   "patience":7,
   "factor":0.5,
   "threshold":0.0001,
   "min_weight":0.2,
   "include_lm":true,
   "lm_shared_softmax":true,
   "lm_schedule":{
      "patience":2,
      "factor":0.5,
      "weight":0.2,
      "mode":"min"
   },
   "batch_size":128,
   "epochs":100,
   "dropout":0.25,
   "word_dropout":0,
   "lr":0.001,
   "lr_patience":4,
   "optimizer":"Adam",
   "clip_norm":5,
   "linear_layers":1,
   "hidden_size":256,
   "num_layers":1,
   "cell":"GRU",
   "wemb_dim":0,
   "merge_type":"concat",
   "cemb_dim":400,
   "cemb_type":"rnn",
   "cemb_layers":2,
   "decoder_layers":3,
   "custom_cemb_cell":false,
   "checks_per_epoch":1,
   "report_freq":200,
   "verbose":true,
   "device":"cuda",
   "buffer_size":10000,
   "minimize_pad":false,
   "shuffle":true,
   "pretrain_embeddings":false,
   "load_pretrained_embeddings":"",
   "load_pretrained_encoder":"",
   "freeze_embeddings":false,
   "scorer":"general",
   "cache_dataset": true
}
